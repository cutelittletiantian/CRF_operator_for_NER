{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CRF算子 (CRF Operator)\n",
    "\n",
    "This CRF operator is simpler than the implementation on PyTorch (https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html?highlight=ner), because I don't use START_TAG and STOP_TAG. So you can focus more on the actual tags you are trying to predict.\n",
    "\n",
    "* mainly used for NER problem\n",
    "* In: emission score matrix\n",
    "    * size of row: count of words in a sentence decided by the length of sentence (no need to consider);\n",
    "    * size of column: the target entity type sequence (set through tag_size during initialization)\n",
    "* Out: tuple (best_score, best_path)\n",
    "    * *best_score* refers to the score calculated according to CRF algorithm\n",
    "    * *best_path* refers to the predicted types of entity, which is a sequence\n",
    "* Loss function for training: *loss_nll_crf(emission, tags)*\n",
    "    * This self-defined loss function (specifically for CRF model) will be called during training process\n",
    "\n",
    "If you have any troubleshooting using this operator, DO let me know. Thank you!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# torch.manual_seed(3407) is all you need: On the influence of random seeds in DL architectures for computer vision\n",
    "# Well, I just set 3407 for fun, nothing serious ...\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, tag_size):\n",
    "        super(CRF, self).__init__()\n",
    "\n",
    "        # real size of tags from input\n",
    "        self.tag_size = tag_size\n",
    "\n",
    "        # PARAM: transitional matrix - element (i,j) refers to the score transitioning from i to j\n",
    "        self.transition = nn.Parameter(\n",
    "            data=torch.tensor(data=np.random.randn(tag_size, tag_size), dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "    def _score_real_path(self, emission, tags):\n",
    "        \"\"\"\n",
    "        A capsuled internal logic for forward() API. DO NOT call this function externally!!!\n",
    "        This function is used for calculating the score of real path given by dataset\n",
    "        :param emission: emission matrix, size: (seq_len, tag_size)\n",
    "        :param tags: the given tags in the dataset, size: (seq_len), where tags[i] represents word_i's real tag\n",
    "        :return: current score of the given real path\n",
    "        \"\"\"\n",
    "        score = torch.zeros(size=[1]) + emission[0, tags[0]]\n",
    "        for i, cur_emission in enumerate(emission[1:], start=1):\n",
    "            score = score + cur_emission[tags[i]] + self.transition[tags[i - 1], tags[i]]\n",
    "        return score\n",
    "\n",
    "    def _score_all_paths(self, emission):\n",
    "        \"\"\"\n",
    "        A capsuled internal logic for forward() API. DO NOT call this function externally!!!\n",
    "        This function is used for calculating the TOTAL score of all possible combinations of tags in a given sequence\n",
    "        :param emission: emission matrix, size: (seq_len, tag_size)\n",
    "        :return: TOTAL\n",
    "        \"\"\"\n",
    "        pre_score = emission[0]\n",
    "        for i, cur_emit in enumerate(emission[1:], start=1):\n",
    "            cur_score = pre_score.unsqueeze(dim=1) + cur_emit + self.transition\n",
    "            pre_score = torch.log(torch.sum(torch.exp(cur_score), dim=0))\n",
    "        # then calculate all the elements in pre_score\n",
    "        pre_score = torch.log(torch.sum(torch.exp(pre_score)))\n",
    "        return pre_score\n",
    "\n",
    "    def forward(self, emission):\n",
    "        \"\"\"\n",
    "        Given the emission score of input sequence, find out the best tag sequence (its score & best path)\n",
    "        Algorithm: viterbi decoding\n",
    "        :param emission: emission score matrix, size: (seq_len, tag_size)\n",
    "        :return: (score, tag_seq) - score of the best path, size: (tag_size); and corresponding tag_seq, size: (tag_size)\n",
    "        \"\"\"\n",
    "        # init: The original score is only the score of word0 in emission score\n",
    "        best_scores = torch.clone(emission[0, :])\n",
    "        # init: The previous tag index of current index, where the score of the path to current tag is the best\n",
    "        previous = torch.zeros(size=emission.shape, dtype=torch.int)\n",
    "        previous[0] = -1\n",
    "\n",
    "        for i, cur_emit in enumerate(emission[1:], start=1):\n",
    "            cur_score = best_scores.unsqueeze(dim=1) + cur_emit + self.transition\n",
    "            best_scores, previous_idx = torch.max(cur_score, dim=0)\n",
    "            # best_scores can be updated automatically, but max_previous should be preserved\n",
    "            previous[i] = previous_idx\n",
    "\n",
    "        # for the final status of best_scores, grab the max score and its current index\n",
    "        best_score, best_idx = best_scores.max(dim=0)\n",
    "        best_score = best_score.tolist()\n",
    "        best_idx = best_idx.tolist()\n",
    "        # find out the best path from prevoius\n",
    "        best_path = [best_idx]\n",
    "        for i in range(previous.shape[0] - 1, 0, -1):\n",
    "            best_idx = previous[i, best_idx].item()\n",
    "            best_path.append(best_idx)\n",
    "        # as a stack, reverse the best_path\n",
    "        best_path = best_path[::-1]\n",
    "\n",
    "        return best_score, best_path\n",
    "\n",
    "    def loss_nll_crf(self, emission, tags):\n",
    "        \"\"\"\n",
    "        Self-defined loss function specifically for CRF. (Negtive Log Likelihood)\n",
    "        Normally, the loss function should be integrated with other DL semantic encoding layer.\n",
    "        :param emission: given emission score from\n",
    "        :param tags: given real tags of the sentence from dataset\n",
    "        :return: Loss of the CRF = log(sum(exp(all_the_paths))) - score_of_real_path\n",
    "        \"\"\"\n",
    "        return self._score_all_paths(emission) - self._score_real_path(emission, tags)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=[8, 16])\n",
    "X.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "crf_op = CRF(tag_size=X.shape[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    best_score, best_path = crf_op(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(20.65096295615288, [14, 14, 0, 2, 1, 12, 8, 7])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22.604530311438175, [11, 12, 1, 2, 3, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# build the model\n",
    "crf_op = CRF(tag_size=X.shape[1])\n",
    "\n",
    "# make up some toy data (you should use real data during training process)\n",
    "training_data = [\n",
    "    (X, [0, 1, 2, 3, 4, 5, 6, 7]),\n",
    "    (torch.randn(size=[8, 16]), [8, 9, 10, 11, 12, 13, 14, 15]),\n",
    "]\n",
    "\n",
    "# create an optimizer\n",
    "optimizer = optim.SGD(crf_op.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# train the model\n",
    "# normally you would NOT do 300 epochs, This is only toy data!!!\n",
    "for epoch in range(300):\n",
    "    for emission, tags in training_data:\n",
    "        crf_op.zero_grad()\n",
    "        loss = crf_op.loss_nll_crf(X, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    print(crf_op(training_data[0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26.06646110200638, [11, 12, 10, 11, 12, 13, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(crf_op(training_data[1][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
