{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CRF算子 (CRF Operator)\n",
    "\n",
    "This CRF operator is simpler than the implementation on PyTorch (https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html?highlight=ner), because I don't use START_TAG and STOP_TAG. So you can focus more on the actual tags you are trying to predict.\n",
    "\n",
    "* mainly used for NER problem\n",
    "* In: emission score matrix\n",
    "    * size of row: count of words in a sentence decided by the length of sentence (no need to consider);\n",
    "    * size of column: the target entity type sequence (set through tag_size during initialization)\n",
    "* Out: tuple (best_score, best_path)\n",
    "    * *best_score* refers to the score calculated according to CRF algorithm\n",
    "    * *best_path* refers to the predicted types of entity, which is a sequence\n",
    "* Loss function for training: *loss_nll_crf(emission, tags)*\n",
    "    * This self-defined loss function (specifically for CRF model) will be called during training process\n",
    "\n",
    "If you have any troubleshooting using this operator, DO let me know. Thank you!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# torch.manual_seed(3407) is all you need: On the influence of random seeds in DL architectures for computer vision\n",
    "# Well, I just set 3407 for fun, nothing serious ...\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, tag_size):\n",
    "        super(CRF, self).__init__()\n",
    "\n",
    "        # real size of tags from input\n",
    "        self.tag_size = tag_size\n",
    "\n",
    "        # PARAM: transitional matrix - element (i,j) refers to the score transitioning from i to j\n",
    "        self.transition = nn.Parameter(\n",
    "            data=torch.tensor(data=np.random.randn(tag_size, tag_size), dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "    def _score_real_path(self, emission, tags):\n",
    "        \"\"\"\n",
    "        A capsuled internal logic for forward() API. DO NOT call this function externally!!!\n",
    "        This function is used for calculating the score of real path given by dataset\n",
    "        :param emission: emission matrix, size: (seq_len, tag_size)\n",
    "        :param tags: the given tags in the dataset, size: (seq_len), where tags[i] represents word_i's real tag\n",
    "        :return: current score of the given real path\n",
    "        \"\"\"\n",
    "        score = emission[0, tags[0]]\n",
    "        for i, cur_emission in enumerate(emission[1:], start=1):\n",
    "            score = score + cur_emission[tags[i]] + self.transition[tags[i - 1], tags[i]]\n",
    "        return score\n",
    "\n",
    "    def _score_all_paths(self, emission):\n",
    "        \"\"\"\n",
    "        A capsuled internal logic for forward() API. DO NOT call this function externally!!!\n",
    "        This function is used for calculating the TOTAL score of all possible combinations of tags in a given sequence\n",
    "        :param emission: emission matrix, size: (seq_len, tag_size)\n",
    "        :return: TOTAL score of all the possible combinations of tags\n",
    "        \"\"\"\n",
    "        pre_score = torch.zeros(size=[1],\n",
    "                                device=f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\") + \\\n",
    "                    emission[0]\n",
    "\n",
    "        for i, cur_emit in enumerate(emission[1:], start=1):\n",
    "            cur_score = pre_score.unsqueeze(dim=1) + cur_emit + self.transition\n",
    "            pre_score = torch.log(torch.sum(torch.exp(cur_score), dim=0))\n",
    "        # then calculate all the elements in pre_score\n",
    "        pre_score = torch.log(torch.sum(torch.exp(pre_score)))\n",
    "        return pre_score\n",
    "\n",
    "    def _viterbi_decode(self, emission):\n",
    "        \"\"\"\n",
    "        A capsuled internal logic for forward() API. DO NOT call this function externally!!!\n",
    "        Given the emission score of input sequence, find out the best tag sequence (its score & best path)\n",
    "        Algorithm: viterbi decoding\n",
    "        :param emission: emission score matrix, size: (seq_len, tag_size)\n",
    "        else size: (batch_size, seq_len, tag_size)\n",
    "        :return: (score, tag_seq) - score of the best path, size: (tag_size); and corresponding tag_seq, size: (tag_size)\n",
    "        \"\"\"\n",
    "        # init: The original score is only the score of word0 in emission score\n",
    "        best_scores = torch.clone(emission[0, :])\n",
    "        # init: The previous tag index of current index, where the score of the path to current tag is the best\n",
    "        previous = torch.zeros(size=emission.shape, dtype=torch.int)\n",
    "        previous[0] = -1\n",
    "\n",
    "        for i, cur_emit in enumerate(emission[1:], start=1):\n",
    "            cur_score = best_scores.unsqueeze(dim=1) + cur_emit + self.transition\n",
    "            best_scores, previous_idx = torch.max(cur_score, dim=0)\n",
    "            # best_scores can be updated automatically, but max_previous should be preserved\n",
    "            previous[i] = previous_idx\n",
    "\n",
    "        # for the final status of best_scores, grab the max score and its current index\n",
    "        best_score, best_idx = best_scores.max(dim=0)\n",
    "        best_score = best_score.tolist()\n",
    "        best_idx = best_idx.tolist()\n",
    "        # find out the best path from prevoius\n",
    "        best_path = [best_idx]\n",
    "        for i in range(previous.shape[0] - 1, 0, -1):\n",
    "            best_idx = previous[i, best_idx].item()\n",
    "            best_path.append(best_idx)\n",
    "        # as a stack, reverse the best_path\n",
    "        best_path = best_path[::-1]\n",
    "        return best_score, best_path\n",
    "\n",
    "    def forward(self, emissions, batch_valid_lens=None, return_tensor=True):\n",
    "        \"\"\"\n",
    "        Given the emission score of input sequence, find out the best tag sequence (its score & best path)\n",
    "        Algorithm: viterbi decoding\n",
    "        :param emissions: emission score matrix, size: (seq_len, tag_size) if batch_valid_lens is not given,\n",
    "        else size: (batch_size, seq_len, tag_size)\n",
    "        :param batch_valid_lens: (optional) valid length of each sentence given in a batch, default None\n",
    "        :return: (score, tag_seq) - score of the best path, size: (tag_size); and corresponding tag_seq, size: (tag_size)\n",
    "        :param return_tensor: If True, the operator will return tensor formed data. Otherwise, return traditional python numeric type and a list type. default: True\n",
    "        \"\"\"\n",
    "        if batch_valid_lens is None:\n",
    "            assert emissions.dim() == 2\n",
    "            best_scores, best_paths = self._viterbi_decode(emissions)\n",
    "        else:  # use batch\n",
    "            assert len(emissions) == len(batch_valid_lens)\n",
    "            best_scores, best_paths = [], []\n",
    "            # eliminate unwanted paddings before decoding\n",
    "            for i, emission in enumerate(emissions):\n",
    "                emit = emission[:batch_valid_lens[i], :]\n",
    "                best_score, best_path = self._viterbi_decode(emit)\n",
    "                best_scores.append(best_score)\n",
    "                best_paths.append(best_path)\n",
    "\n",
    "        if return_tensor:\n",
    "            best_scores = torch.tensor(best_scores)\n",
    "            if type(best_paths[0]) == list:\n",
    "                best_paths = [torch.tensor(bp) for bp in best_paths]\n",
    "            else:\n",
    "                best_paths = torch.tensor(best_paths, dtype=torch.int)\n",
    "        else:\n",
    "            pass\n",
    "        return best_scores, best_paths\n",
    "\n",
    "    def loss_nll_crf(self, emissions, tags):\n",
    "        \"\"\"\n",
    "        Self-defined loss function specifically for CRF. (Negtive Log Likelihood)\n",
    "        Normally, the loss function should be integrated with other DL semantic encoding layer.\n",
    "        :param emissions: given emission score from\n",
    "        :param tags: given real tags of the sentence from dataset\n",
    "        :return: Loss of the CRF = log(sum(exp(all_the_paths))) - score_of_real_path\n",
    "        \"\"\"\n",
    "        return self._score_all_paths(emissions) - self._score_real_path(emissions, tags)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# set up device\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For CRF, normally, X refers to the emission matrix.\n",
    "# For demo purpose, here I initialize it randomly.\n",
    "# But you should use careful feature engineering method OR find out emission matrix from other DL model\n",
    "X = torch.randn(size=[8, 5]).to(device)\n",
    "X.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "crf_op = CRF(tag_size=X.shape[1]).to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    best_score, best_path = crf_op(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(13.4126), tensor([3, 4, 3, 2, 2, 2, 2, 4], dtype=torch.int32))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "a simple reference for showing how to train the model with batched data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# specify your device (CUDA supported)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# make up a group of batch data as some toy data (you should use real data during training process)\n",
    "valid_lens = [8, 4, 5]\n",
    "training_data = [\n",
    "    ((torch.randn(size=[3, 8, 5]).to(device), torch.tensor(valid_lens)),\n",
    "     torch.tensor([[1, 2, 0, 3, 2, 4, 3, 0], [1, 4, 2, 3, 5, 5, 5, 5], [0, 1, 3, 2, 4, 5, 5, 5]])\n",
    "     )\n",
    "]\n",
    "\n",
    "# build the model\n",
    "crf_op = CRF(tag_size=5).to(device=device)\n",
    "\n",
    "# create an optimizer\n",
    "optimizer = optim.SGD(crf_op.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "\n",
    "# train the model\n",
    "# normally you would NOT do 5000 epochs, This is only toy data!!!\n",
    "for epoch in range(15000):\n",
    "    for sentence_idx, tags in training_data:\n",
    "        emissions, valid_lengths = sentence_idx\n",
    "        # loss: use self-defined nll loss function for CRF model\n",
    "        for i, emit in enumerate(emissions):\n",
    "            loss = crf_op.loss_nll_crf(emit[:valid_lengths[i], :], tags[i][:valid_lengths[i]])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29.1700, 14.3275, 20.0263])\n",
      "[tensor([2, 4, 3, 2, 3, 2, 4, 2]), tensor([1, 2, 0, 3]), tensor([1, 3, 0, 1, 4])]\n"
     ]
    }
   ],
   "source": [
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    score, labels = crf_op(training_data[0][0][0], batch_valid_lens=training_data[0][0][1])\n",
    "    print(score)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you don't train batched data, here is the reference:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "tensor(42.4312)\n",
      "tensor([1, 2, 4, 3, 0, 3, 2, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# specify your device (CUDA supported)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# make up some toy data (you should use real data during training process)\n",
    "X = torch.randn(size=[8, 5]).to(device)\n",
    "y = [1, 2, 0, 3, 2, 4, 3, 0]\n",
    "training_data = [\n",
    "    (X, y)\n",
    "]\n",
    "\n",
    "# build the model\n",
    "crf_op = CRF(tag_size=5).to(device=device)\n",
    "\n",
    "# create an optimizer\n",
    "optimizer = optim.SGD(crf_op.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "\n",
    "# train the model\n",
    "# normally you would NOT do 5000 epochs, This is only toy data!!!\n",
    "for epoch in range(5000):\n",
    "    for emission, tags in training_data:\n",
    "        # loss: use self-defined nll loss function for CRF model\n",
    "        loss = crf_op.loss_nll_crf(emission, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    score, labels = crf_op(training_data[0][0])\n",
    "    print(score)\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
